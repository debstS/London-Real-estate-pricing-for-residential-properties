# -*- coding: utf-8 -*-
"""London Residential Property Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Diqam_Worfz2DEr20NTSgOva6CDrgwP7
"""

import numpy as np # library to handle data in a vectorized manner

import pandas as pd # library for data analsysis
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

import json # library to handle JSON files

import geocoder as geocoder
from geopy.geocoders import Nominatim # convert an address into latitude and longitude values

import requests # library to handle requests
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe

# Matplotlib and associated plotting modules
import matplotlib.cm as cm
import matplotlib.colors as colors

# import k-means from clustering stage
from sklearn.cluster import KMeans

import folium # map rendering library

print('Libraries imported.')

"""## Web scraping of London Boroughs Data from Wikipedia"""

!pip install wikipedia

import pandas as pd
import wikipedia as wp

#Get the html source
html = wp.page("List of areas of London").html().encode("UTF-8")
df = pd.read_html(html)[1]
df.head()

df.columns

df.columns = ['Location','London_borough','Post town','Postcode district','Dial Code','OS grid ref']
df.columns

"""## DATA CLEANING

The table contains hyperlinks, and special symbols such as square bracket (ex- Greenwich [2]), etc. Also there are more than one Postal code columns, so we will go with keeping only one.
"""

df['London_borough'] =  df['London_borough'].apply(lambda x: x.replace('[','').replace(']',''))
df['London_borough'] =  df['London_borough'].str.replace('\d+', '')


df['London_borough'] =  df['London_borough'].str.split(',').str[0]
df['Postcode district'] =  df['Postcode district'].str.split(',').str[0]
df['Postcode district'] =  df['Postcode district'].str.split('(').str[0]
df['Post town'] =  df['Post town'].str.split(',').str[0]

df.head(5)

"""Saving into a CSV file:-"""

df.to_csv('london.csv',index=False)

df.shape

"""### Scraping Property Data for London to make decisions about areas to buy as a prospective buyer"""

# tables = pd.read_html("https://propertydata.co.uk/cities/london",header=0)
# df2=pd.DataFrame(data=tables[0])
# df2

## Another methoda below as the above doesn't work

import urllib.request

from urllib.request import Request, urlopen

req = Request(
    url='https://propertydata.co.uk/cities/london',
    headers={'User-Agent': 'Mozilla/5.0'}
)
webpage = urlopen(req).read()

#To store the scraped webpage into a Pandas DataFrame

tables = pd.read_html(webpage,header=0)
df2=pd.DataFrame(data=tables[0])
df2.head(5)

"""## Data Cleaning of the Property Data

First, we want to remove unwanted columns. Next, we need to correct price column which is in string format ('£' and commas to be removed)

"""

df2.drop(columns=['Avg rental yield','Avg sold £/sqft','Explore more','Sales per month'],inplace=True)

df2.head()

"""### Removing GBP sign and comma from 'Avg asking price' column"""

df2['Avg asking price'] = df2['Avg asking price'].str.replace("£","")
df2['Avg asking price'] = df2['Avg asking price'].str.replace(',', '')
df2['Avg asking price'] = pd.to_numeric(df2['Avg asking price'])

df2.head(5)

df2.to_csv('london_postcode.csv',index=False)

"""# Merge the 'London Boroughs' and 'London Property Prices' dataframes

We are using inner join here because there were many postcodes that were not in the London Borough's dataframe, and we want to make sure we are fetching prices of only London Boroughs dataframe.
"""

data = pd.merge(df, df2, how='inner', left_on='Postcode district', right_on='Postcode district')

"""# Finding Longitude and Latitude of the Locations

"""

def getLatLong(row):
    # initialize your variable to None
    lat_lng_coords = None
    search_query = '{}, London,UK'.format(row)
    # loop until you get the coordinates
    try:
        while(lat_lng_coords is None):
            #g = geocoder.here(search_query,app_id=app_id,app_code=app_code)
            g = geocoder.arcgis(search_query)
            lat_lng_coords = g.latlng
            #print('FIRST')
    except IndexError:
        latitude = 0.0
        longitude = 0.0
        print('BACKUP')
        return [latitude,longitude]

    latitude = lat_lng_coords[0]
    longitude = lat_lng_coords[1]
    print(latitude, longitude)
    return [latitude, longitude]

coords_list = data['Postcode district'].apply(getLatLong).tolist()

"""## Merging the values in the dataframe


"""

data[['Latitude','Longitude']]=pd.DataFrame(coords_list,columns=['Latitude', 'Longitude'])
data.head()

"""## Droping unwanted columns"""

data.drop(columns=['Post town','Dial Code','OS grid ref'],inplace=True)

"""#### Use geopy library to get the latitude and longitude values of London."""

address = 'London'

geolocator = Nominatim(user_agent="ldn_explorer")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of London are {}, {}.'.format(latitude, longitude))

"""## Create a map of London with neighborhoods superimposed on top."""

map_london = folium.Map(location=[latitude, longitude], zoom_start=10)

# add markers to map
for lat, lng, borough, neighborhood in zip(data['Latitude'], data['Longitude'], data['London_borough'], data['Location']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=2,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_london)

map_london

"""Next, utilizing the Foursquare API to explore the neighborhoods and segment them.

## Define Foursquare Credentials and Version
"""

CLIENT_ID = 'U0OWUYRP10WK5UFHIVQGDEKRQLZBSFYRALYTWAF0OBVJHIZ4' # your Foursquare ID
CLIENT_SECRET = 'R2BLV2WI204CLFJ5GTAS1OR0U2AFMKZV5UTWI20OREP30SZQ' # your Foursquare Secret
VERSION = '20180604'

def getBuiltUrl(neigh_lat,neigh_long,radius=1400):
    # type your answer here
    LIMIT=100
    #radius=1000
    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID,
    CLIENT_SECRET,
    VERSION,
    neigh_lat,
    neigh_long,
    radius,
    LIMIT)
    return url

"""## Exploring first Location in our dataframe"""

neigh_name, neigh_borough, neigh_post, neigh_price, neigh_lat, neigh_long = data.iloc[0]
print('Latitude and longitude values of {} are {}, {}.'.format(neigh_name,
                                                               neigh_lat,
                                                               neigh_long))
results = requests.get(getBuiltUrl(neigh_lat,neigh_long)).json()
results

def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']

    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

"""#### Now cleaning the json file and structure it into a *pandas* dataframe."""

venues = results['response']['groups'][0]['items']

nearby_venues = json_normalize(venues) # flatten JSON

# filter columns
filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]

# filter the category for each row
nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)

# clean columns
nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns]

nearby_venues.head()

print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))

"""# Function to repeat the same process for all the neighborhoods in London::"""

def getNearbyVenues(names, latitudes, longitudes, radius=500):

    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)

        # create the API request URL
        LIMIT=100
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID,
            CLIENT_SECRET,
            VERSION,
            lat,
            lng,
            radius,
            LIMIT)

        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']

        # return only relevant information for each nearby venue
        venues_list.append([(
            name,
            lat,
            lng,
            v['venue']['name'],
            v['venue']['location']['lat'],
            v['venue']['location']['lng'],
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood',
                  'Neighborhood Latitude',
                  'Neighborhood Longitude',
                  'Venue',
                  'Venue Latitude',
                  'Venue Longitude',
                  'Venue Category']

    return(nearby_venues)

"""#### Running our function for all Location in the dataframe"""

london_venues = getNearbyVenues(names=data['Location'],
                                   latitudes=data['Latitude'],
                                   longitudes=data['Longitude'],
                                   radius=500)

print(london_venues.shape)
london_venues.head()

"""### Venues returned for each Neighborhood Location"""

london_venues.groupby('Neighborhood').count()

print('There are {} uniques categories.'.format(len(london_venues['Venue Category'].unique())))

"""## Analyzing each Neighborhood Location"""

# one hot encoding
london_onehot = pd.get_dummies(london_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
london_onehot['Neighborhood'] = london_venues['Neighborhood']

# move neighborhood column to the first column
fixed_columns = [london_onehot.columns[-1]] + list(london_onehot.columns[:-1])
london_onehot = london_onehot[fixed_columns]

london_onehot.head()

london_onehot.shape

"""###  Grouping rows by neighborhood and by taking the mean of the frequency of occurrence of each category"""

london_grouped = london_onehot.groupby('Neighborhood').mean().reset_index()
london_grouped

a=london_grouped
a.head()

"""### Top 5 most common venues for each Neighborhood"""

num_top_venues = 5

for hood in a['Neighborhood']:
    print("----"+hood+"----")
    temp = a[a['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')

"""## Put 'Most Common Venues' in a Dataframe"""

def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)

    return row_categories_sorted.index.values[0:num_top_venues]

"""#### Create the new dataframe and display the top 10 venues for each neighborhood."""

num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = a['Neighborhood']

for ind in np.arange(a.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(a.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()

neighborhoods_venues_sorted.rename(columns = {'Neighborhood':'Location'}, inplace = True)

"""## Adding Average house price of each Neighborhood in the Group
### Normalizing the Avg price column
"""

a['Price']=data['Avg asking price']
v= a.iloc[:, -1]
a.iloc[:,-1] = (v - v.min()) / (v.max() - v.min())

"""


# Clustering Neighborhoods

Running K-means clustering for 4 clusters"""

kclusters = 6

london_grouped_clustering = a.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(london_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10]

from sklearn import metrics
from scipy.spatial.distance import cdist
import numpy as np
import matplotlib.pyplot as plt

# k means determine k
distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k).fit(london_grouped_clustering)
    kmeanModel.fit(london_grouped_clustering)
    distortions.append(sum(np.min(cdist(london_grouped_clustering, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / london_grouped_clustering.shape[0])

# Plot the elbow
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

#neighborhoods_venues_sorted.drop(columns = 'Cluster Labels', inplace = True)

"""#### Create a new dataframe that includes the cluster as well as the top 10 venues for each neighborhood."""

neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

london_merged = data

london_merged = london_merged.join(neighborhoods_venues_sorted.set_index('Location'), on='Location')

london_merged

"""##### There was null value in the merged data so droping the rows with null values"""

london_merged.dropna(inplace=True)

"""##### Because of null values the data type of 'Cluster Labels' was changed from int to float so converting it back to int"""

london_merged['Cluster Labels'] = london_merged['Cluster Labels'].astype(int)
london_merged.dtypes

"""### There were many Location that were assigned the same postcodes as they were very near, hence droping the duplicate postcodes"""

london_merged.drop_duplicates(subset='Postcode district',inplace=True)

london_merged.reset_index(inplace=True)
london_merged.drop(columns='index',inplace=True)

"""# Visualize the clusters"""

map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(london_merged['Latitude'],london_merged['Longitude'], london_merged['Location'], london_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=3,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)

map_clusters

"""# Binning

### There is the range of Avg price so binned the price into 7 distinct values
##### ('Low level 1', 'Low level 2', 'Average level 1', 'Average level 2','Above Average','High level 1','High level 2')

Visualizing the bins
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(london_merged["Avg asking price"],bins=7)

# set x/y labels and plot title
plt.pyplot.xlabel("Avg asking price")
plt.pyplot.ylabel("count")
plt.pyplot.title("Price bins")

bins = np.linspace(min(london_merged["Avg asking price"]), max(london_merged["Avg asking price"]), 8)
bins

group_names = ['Low level 1', 'Low level 2', 'Average level 1', 'Average level 2','Above Average','High level 1','High level 2']

london_merged['Price-Categories'] = pd.cut(london_merged['Avg asking price'], bins, labels=group_names, include_lowest=True )
london_merged[['Avg asking price','Price-Categories']].head()

"""## Cluster bins
### Creating 4 bins for clusters
"""

plt.pyplot.hist(london_merged["Cluster Labels"],bins=4)

# set x/y labels and plot title
plt.pyplot.xlabel("Cluster Labels")
plt.pyplot.ylabel("count")
plt.pyplot.title("Cluster Labels")

bins = np.linspace(min(london_merged["Cluster Labels"]), max(london_merged["Cluster Labels"]), 7)
bins

group_names = ['Mixed Social Venues','Hotels and Social Venues','Stores and seafood restaurants','Pubs and Historic places', 'Sports and Athletics','Restaurants and Bars']

london_merged['Cluster-Category'] = pd.cut(london_merged['Cluster Labels'], bins, labels=group_names, include_lowest=True )
london_merged[['Cluster Labels','Cluster-Category']].head()

"""# Final Data"""

london_merged.drop(columns=['6th Most Common Venue','7th Most Common Venue','8th Most Common Venue','9th Most Common Venue','10th Most Common Venue'],inplace=True)
london_merged.head(20)

"""# Creating Choropleth map to visualize how London is divided in terms of Housing prices and cluster markers on the top"""

lnd_geo = r'london_boroughs_proper.geojson'
lnd_map = folium.Map(location = [latitude, longitude], zoom_start = 11)

lnd_map.choropleth(
    geo_data=lnd_geo,
    data=london_merged,
    columns=['London_borough','Avg asking price'],
    key_on='feature.properties.name',
    fill_color='RdPu',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Average house Prices'
)
# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(london_merged['Latitude'],london_merged['Longitude'], london_merged['Location'], london_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=3,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(lnd_map)

# display map
lnd_map

"""# Chloropleth map so to see how the London is clustered"""

lnd_geo = r'london_boroughs_proper.geojson'
lnd_map = folium.Map(location = [latitude, longitude], zoom_start = 10)

lnd_map.choropleth(
    geo_data=lnd_geo,
    data=london_merged,
    columns=['London_borough','Cluster Labels'],
    key_on='feature.properties.name',
    fill_color='PuRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Cluster Labels'
)

lnd_map

"""# Examining the Clusters

## Cluster 1
"""

london_merged[london_merged['Cluster Labels']==0]

"""## Cluster 2"""

london_merged[london_merged['Cluster Labels']==1]

"""## Cluster 3"""

london_merged[london_merged['Cluster Labels']==2]

"""## Cluster 4"""

london_merged[london_merged['Cluster Labels']==3]

"""## Cluster 5"""

london_merged[london_merged['Cluster Labels']==4]

"""## Cluster 6"""

london_merged[london_merged['Cluster Labels']==5]

"""# Examining Property prices"""

london_merged[london_merged['Price-Categories']=='High level 2']

london_merged[london_merged['Price-Categories']=='High level 1']

london_merged[london_merged['Price-Categories']=='Low level 2']

london_merged[london_merged['Price-Categories']=='Low level 1']